---
description: 
globs: 
alwaysApply: false
---
# Rule: Comprehensive Task List Quality Evaluation Framework

## Goal

Guide an AI assistant in conducting systematic, multi-dimensional quality evaluation of task lists generated from PRDs. This evaluation ensures task lists are implementation-ready, properly sequenced, adequately detailed, and contain sufficient validation checkpoints to guarantee successful feature delivery.

## Process Overview

1. **Receive Task List for Evaluation:** User provides path to task list file or requests evaluation after generation
2. **Structural & Flow Analysis:** Assess task organization, dependencies, and logical progression
3. **Implementation Readiness Assessment:** Evaluate if tasks provide sufficient guidance for execution
4. **Risk & Gap Identification:** Find missing tasks, unclear instructions, inadequate validation
5. **Quality Scoring & Reporting:** Quantified assessment with specific improvement recommendations
6. **Iterative Refinement:** Guide user through addressing identified issues

## Core Philosophy

- **Implementation Success Focus:** Tasks must lead to successful feature delivery
- **Developer Experience Priority:** Tasks should reduce cognitive load, not increase it
- **Validation-Heavy Approach:** Every significant change must have validation steps
- **Risk Mitigation Embedded:** Potential failures should be anticipated and prevented
- **Adaptable Complexity:** Task detail should match feature complexity and team experience

## Task List Quality Framework

### Dimension 1: Structural Organization & Flow

#### Task Hierarchy Assessment
**Excellent (4/4):**
- Clear phase separation with logical boundaries
- Consistent numbering scheme throughout
- Proper nesting of sub-tasks under parent tasks
- Dependencies explicitly identified and respected
- Critical path clearly identifiable

**Good (3/4):**
- Generally well-organized with minor hierarchy issues
- Numbering mostly consistent with few gaps
- Most dependencies clear but some implicit
- Phase boundaries mostly logical
- Critical path identifiable with some analysis

**Needs Improvement (2/4):**
- Inconsistent organization and numbering
- Some tasks in wrong phases or poorly grouped
- Many implicit dependencies not explicitly stated
- Unclear critical path
- Mixing of different abstraction levels

**Poor (1/4):**
- No clear organizational structure
- Inconsistent or missing numbering
- Tasks randomly ordered without logical flow
- Dependencies unclear or missing
- No identifiable phases or progression

#### Dependency Management
**Evaluate:**
- [ ] **Sequential Dependencies:** Are tasks that must be done in order clearly marked?
- [ ] **Parallel Opportunities:** Are tasks that can be done simultaneously identified?
- [ ] **External Dependencies:** Are dependencies on other teams/systems explicitly noted?
- [ ] **Resource Dependencies:** Are tool, environment, or skill dependencies identified?
- [ ] **Blocking Relationships:** Are potential blockers and their resolutions planned?

#### Phase Structure Analysis
**Check for:**
- [ ] **Logical Phase Boundaries:** Do phases represent meaningful implementation milestones?
- [ ] **Phase Completeness:** Does each phase have clear deliverables and success criteria?
- [ ] **Prerequisites Clarity:** Are phase prerequisites explicitly stated?
- [ ] **Checkpoint Integration:** Are validation checkpoints strategically placed?
- [ ] **Rollback Considerations:** Can work be safely rolled back at phase boundaries?

### Dimension 2: Task Detail & Clarity

#### Task Specification Quality
**Excellent (4/4):**
- Every task has clear, actionable verbs (implement, test, validate, configure)
- Specific file paths, function names, or components mentioned
- Expected outcomes explicitly defined
- Time estimates realistic and justified
- Success criteria measurable and objective

**Good (3/4):**
- Most tasks actionable with minor ambiguities
- Generally specific with some vague language
- Outcomes mostly clear but some assumptions required
- Time estimates reasonable but not always justified
- Success criteria mostly measurable

**Needs Improvement (2/4):**
- Many tasks use vague language ("improve," "enhance," "optimize")
- Missing specific implementation details
- Outcomes unclear or subjective
- Time estimates unrealistic or missing
- Success criteria vague or unmeasurable

**Poor (1/4):**
- Tasks consistently vague and non-actionable
- No specificity about what to implement or change
- No clear outcomes defined
- No time estimates or completely unrealistic ones
- No measurable success criteria

#### Implementation Guidance Assessment
**For Each Task, Evaluate:**
- **Actionability:** Can a developer start working immediately after reading this task?
- **Specificity:** Are file names, API endpoints, database tables specifically mentioned?
- **Context:** Is enough background provided to understand why this task is needed?
- **Constraints:** Are technical limitations, performance requirements, or business rules specified?
- **Examples:** Are code patterns, API formats, or data structures provided where helpful?

#### Acceptance Criteria Quality
**Check for:**
- [ ] **Testable Criteria:** Can each criterion be verified objectively?
- [ ] **Completeness:** Do criteria cover functionality, performance, security, usability?
- [ ] **Edge Cases:** Are error conditions and boundary cases addressed?
- [ ] **Integration Points:** Are system interactions validated?
- [ ] **User Impact:** Are user-facing changes validated from user perspective?

### Dimension 3: Risk Management & Validation

#### Risk Coverage Assessment
**Technical Risk Mitigation:**
- [ ] **Data Safety:** Are database changes protected with backups/migrations?
- [ ] **Performance Impact:** Are performance baselines and monitoring included?
- [ ] **Integration Failures:** Are external API failures and timeouts handled?
- [ ] **Security Vulnerabilities:** Are security reviews and testing included?
- [ ] **Deployment Risks:** Are deployment validation and rollback procedures defined?

**Implementation Risk Mitigation:**
- [ ] **Scope Creep:** Are boundaries clearly defined to prevent feature expansion?
- [ ] **Technical Debt:** Are code quality checks and refactoring tasks included?
- [ ] **Knowledge Gaps:** Are learning/research tasks included for unfamiliar technologies?
- [ ] **Resource Constraints:** Are team capacity and skill requirements realistic?
- [ ] **Timeline Pressure:** Are buffers included for unexpected complexity?

#### Validation Strategy Completeness
**Testing Coverage:**
- [ ] **Unit Tests:** Are unit testing tasks specific about what to test?
- [ ] **Integration Tests:** Are all system integration points covered?
- [ ] **End-to-End Tests:** Are critical user journeys automated?
- [ ] **Performance Tests:** Are load, stress, and endurance tests defined?
- [ ] **Security Tests:** Are vulnerability scans and penetration tests included?

**Quality Assurance:**
- [ ] **Code Review:** Are review checkpoints strategically placed?
- [ ] **Documentation Review:** Are documentation updates validated?
- [ ] **User Acceptance:** Are UAT scenarios and stakeholder sign-offs included?
- [ ] **Accessibility Testing:** Are accessibility requirements validated?
- [ ] **Cross-Browser/Platform:** Are compatibility tests defined?

### Dimension 4: Implementation Readiness

#### Developer Experience Assessment
**Task Execution Ease:**
- **Environment Setup:** Are development environment requirements specified?
- **Tool Requirements:** Are necessary tools, libraries, and dependencies listed?
- **Reference Materials:** Are links to documentation, APIs, and examples provided?
- **Setup Instructions:** Are one-time setup tasks clearly separated from recurring tasks?
- **Troubleshooting:** Are common issues and their solutions documented?

**Knowledge Transfer:**
- **Context Preservation:** Is sufficient background provided for future maintainers?
- **Decision Rationale:** Are architectural and design decisions explained?
- **Pattern Documentation:** Are coding patterns and conventions specified?
- **Integration Guidance:** Are API contracts and data formats documented?

#### Team Scalability Assessment
**Multi-Developer Readiness:**
- [ ] **Parallel Work:** Can multiple developers work simultaneously without conflicts?
- [ ] **Work Distribution:** Are tasks sized appropriately for individual contributors?
- [ ] **Skill Matching:** Are tasks matched to appropriate skill levels?
- [ ] **Knowledge Sharing:** Are knowledge transfer tasks included?
- [ ] **Code Integration:** Are merge conflict prevention strategies included?

#### Handoff Preparation
**Operational Readiness:**
- [ ] **Deployment Procedures:** Are production deployment steps documented?
- [ ] **Monitoring Setup:** Are logging, metrics, and alerting tasks included?
- [ ] **Documentation Creation:** Are user and technical documentation tasks defined?
- [ ] **Support Preparation:** Are runbooks and troubleshooting guides planned?
- [ ] **Training Requirements:** Are stakeholder training tasks included?

### Dimension 5: Completeness & Coverage

#### Feature Implementation Coverage
**Functional Completeness:**
- [ ] **Core Functionality:** Are all primary use cases implemented?
- [ ] **Edge Case Handling:** Are boundary conditions and error scenarios covered?
- [ ] **Integration Points:** Are all external system interactions implemented?
- [ ] **Data Handling:** Are CRUD operations, validation, and transformation complete?
- [ ] **User Interface:** Are all UI components and interactions implemented?

**Non-Functional Coverage:**
- [ ] **Performance:** Are optimization and monitoring tasks included?
- [ ] **Security:** Are authentication, authorization, and data protection implemented?
- [ ] **Scalability:** Are load handling and resource management addressed?
- [ ] **Reliability:** Are error handling and recovery mechanisms implemented?
- [ ] **Maintainability:** Are code quality and documentation tasks included?

#### Lifecycle Completeness
**Development Lifecycle:**
- [ ] **Analysis & Design:** Are research and design tasks included?
- [ ] **Implementation:** Are coding tasks specific and complete?
- [ ] **Testing:** Are all levels of testing adequately covered?
- [ ] **Integration:** Are system integration tasks defined?
- [ ] **Deployment:** Are deployment and go-live tasks specified?

**Maintenance Preparation:**
- [ ] **Documentation:** Are all documentation deliverables identified?
- [ ] **Monitoring:** Are operational monitoring tasks included?
- [ ] **Support:** Are support process and tool setup tasks defined?
- [ ] **Optimization:** Are performance tuning and improvement tasks planned?

### Dimension 6: Adaptability & Future-Proofing

#### Flexibility Assessment
**Change Accommodation:**
- **Requirement Changes:** Can tasks be modified without complete restructuring?
- **Technical Changes:** Can implementation approach be adjusted mid-stream?
- **Resource Changes:** Can task assignments be redistributed if needed?
- **Timeline Changes:** Can phases be accelerated or decelerated as needed?
- **Scope Changes:** Can features be added or removed cleanly?

**Scalability Considerations:**
- **Team Growth:** Can additional developers be onboarded easily?
- **Complexity Growth:** Can tasks handle discovered complexity gracefully?
- **Integration Expansion:** Can additional integrations be added without major rework?
- **Feature Evolution:** Are tasks structured to support future enhancements?

## Evaluation Process

### Phase 1: Initial Structure Analysis

1. **Document Overview**
   - Identify task list structure and organization approach
   - Count phases, major tasks, and sub-tasks
   - Assess overall complexity and scope
   - Check for adherence to task list template standards

2. **Flow Analysis**
   - Trace critical path through task list
   - Identify parallel work opportunities
   - Verify dependency logic and sequencing
   - Check for circular dependencies or impossible sequences

### Phase 2: Deep Quality Assessment

Execute systematic evaluation across all six dimensions, documenting specific findings with task references.

### Phase 3: Gap Analysis

**Missing Task Categories:**
- [ ] **Environment Setup:** Development environment preparation
- [ ] **Data Migration:** Existing data handling and migration scripts
- [ ] **API Documentation:** Endpoint documentation and examples
- [ ] **Error Handling:** Comprehensive error scenario handling
- [ ] **Performance Benchmarking:** Baseline establishment and monitoring
- [ ] **Security Scanning:** Vulnerability assessment and penetration testing
- [ ] **Load Testing:** Performance validation under expected load
- [ ] **Rollback Procedures:** Feature toggle and deployment rollback
- [ ] **User Training:** End-user documentation and training materials
- [ ] **Operational Handoff:** Production support preparation

**Missing Validation Points:**
- [ ] **Code Quality Checks:** Linting, formatting, complexity analysis
- [ ] **Test Coverage Validation:** Coverage thresholds and gap analysis
- [ ] **Performance Regression:** Comparison against baseline metrics
- [ ] **Security Compliance:** Regulatory and internal security standards
- [ ] **Accessibility Validation:** WCAG compliance and usability testing
- [ ] **Browser/Platform Compatibility:** Cross-environment validation
- [ ] **Integration Testing:** End-to-end system validation
- [ ] **User Acceptance Testing:** Stakeholder approval and sign-off

### Phase 4: Implementation Risk Assessment

**High-Risk Task Patterns:**
- Tasks that modify core system components without adequate testing
- Database schema changes without proper migration and rollback procedures
- External API integrations without timeout and failure handling
- Performance-critical code without benchmarking and monitoring
- Security-sensitive operations without security review checkpoints

**Missing Risk Mitigations:**
- Insufficient testing for critical functionality changes
- Lack of rollback procedures for high-risk modifications
- Missing monitoring and alerting for new system components
- Inadequate error handling for external dependencies
- No performance validation for user-facing changes

## Task List Quality Scorecard

### Overall Quality Score Calculation

```
Total Score = (Structure × 0.20) + (Task Detail × 0.25) + (Risk Management × 0.20) + 
              (Implementation Readiness × 0.15) + (Completeness × 0.15) + (Adaptability × 0.05)

Score Interpretation:
- 3.5-4.0: Excellent - Ready for immediate implementation
- 3.0-3.4: Good - Minor improvements recommended
- 2.5-2.9: Adequate - Several improvements needed before implementation
- 2.0-2.4: Poor - Significant rework required
- Below 2.0: Unacceptable - Complete task list revision needed
```

### Detailed Scoring Breakdown

**Structural Organization & Flow (20%)**
- Task Hierarchy: __/4
- Dependency Management: __/4
- Phase Structure: __/4
- Weighted Score: __/4

**Task Detail & Clarity (25%)**
- Task Specification Quality: __/4
- Implementation Guidance: __/4
- Acceptance Criteria: __/4
- Weighted Score: __/4

**Risk Management & Validation (20%)**
- Risk Coverage: __/4
- Validation Strategy: __/4
- Weighted Score: __/4

**Implementation Readiness (15%)**
- Developer Experience: __/4
- Team Scalability: __/4
- Handoff Preparation: __/4
- Weighted Score: __/4

**Completeness & Coverage (15%)**
- Feature Implementation: __/4
- Lifecycle Coverage: __/4
- Weighted Score: __/4

**Adaptability & Future-Proofing (5%)**
- Flexibility: __/4
- Scalability Considerations: __/4
- Weighted Score: __/4

## Evaluation Report Template

```markdown
# Task List Quality Evaluation Report

**Document:** [Task list filename]
**Evaluated By:** AI Assistant
**Date:** [Current date]
**Overall Score:** X.X/4.0 ([Excellent/Good/Adequate/Poor/Unacceptable])

## Executive Summary

### Implementation Readiness Assessment
**Recommendation:** [Ready for immediate implementation / Needs minor improvements / Requires significant rework / Not ready for implementation]

### Key Strengths
1. [Most significant strength]
2. [Second strength]
3. [Third strength]

### Critical Issues
1. [Most critical issue blocking implementation]
2. [Second critical issue]
3. [Third critical issue]

### Major Recommendations
1. [Top priority improvement]
2. [Second priority improvement]
3. [Third priority improvement]

---

## Detailed Quality Assessment

### 1. Structural Organization & Flow (Score: X.X/4.0)

#### Task Hierarchy Analysis
**Strengths:**
- [Well-organized aspects]

**Issues:**
- [Structural problems with specific task references]

**Recommendations:**
- [Specific structural improvements needed]

#### Dependency Management
**Well-Defined Dependencies:**
- [Tasks with clear dependencies]

**Missing or Unclear Dependencies:**
- [Tasks with dependency issues and recommendations]

#### Phase Structure Assessment
**Logical Phases:**
- [Phases that make sense]

**Problematic Phases:**
- [Phases needing restructuring or clarification]

### 2. Task Detail & Clarity (Score: X.X/4.0)

#### Task Specification Quality
**Clear and Actionable Tasks:**
- [Examples of well-written tasks]

**Vague or Unclear Tasks:**
- Task X.X: "[Task title]" - [What's unclear and how to improve]
- Task X.X: "[Task title]" - [What's unclear and how to improve]

#### Implementation Guidance Assessment
**Sufficient Guidance:**
- [Tasks with adequate implementation details]

**Insufficient Guidance:**
- [Tasks needing more implementation detail]

#### Acceptance Criteria Evaluation
**Well-Defined Criteria:**
- [Tasks with clear acceptance criteria]

**Missing or Vague Criteria:**
- [Tasks needing better acceptance criteria]

### 3. Risk Management & Validation (Score: X.X/4.0)

#### Risk Coverage Analysis
**Well-Mitigated Risks:**
- [Risks that are properly addressed]

**Unaddressed Risks:**
- [Risk]: [Why it's concerning] - [Recommended mitigation]
- [Risk]: [Why it's concerning] - [Recommended mitigation]

#### Validation Strategy Assessment
**Adequate Validation:**
- [Areas with sufficient testing/validation]

**Validation Gaps:**
- [Missing validation tasks and recommendations]

### 4. Implementation Readiness (Score: X.X/4.0)

#### Developer Experience Analysis
**Positive Aspects:**
- [What makes tasks easy to implement]

**Developer Experience Issues:**
- [What makes tasks difficult to implement]

#### Team Scalability Assessment
**Parallelization Opportunities:**
- [Tasks that can be done in parallel]

**Bottlenecks:**
- [Tasks that create development bottlenecks]

### 5. Completeness & Coverage (Score: X.X/4.0)

#### Feature Implementation Coverage
**Well Covered:**
- [Aspects of the feature that are comprehensively addressed]

**Coverage Gaps:**
- [Missing functionality or implementation areas]

#### Lifecycle Coverage Assessment
**Complete Lifecycle Stages:**
- [Development stages that are well covered]

**Missing Lifecycle Elements:**
- [Development stages that need additional tasks]

### 6. Adaptability & Future-Proofing (Score: X.X/4.0)

#### Flexibility Analysis
**Adaptable Aspects:**
- [How the task list can accommodate changes]

**Rigid Aspects:**
- [Areas that would be difficult to modify]

---

## Missing Task Analysis

### Critical Missing Tasks
1. **[Task Category]**: [Why it's critical] - [Specific tasks needed]
2. **[Task Category]**: [Why it's critical] - [Specific tasks needed]

### Important Missing Tasks
1. **[Task Category]**: [Why it's important] - [Specific tasks needed]
2. **[Task Category]**: [Why it's important] - [Specific tasks needed]

### Recommended Additional Tasks
1. **[Task Category]**: [Why it's beneficial] - [Specific tasks suggested]
2. **[Task Category]**: [Why it's beneficial] - [Specific tasks suggested]

---

## Specific Task Improvement Recommendations

### Tasks Needing Immediate Attention

#### Task X.X: "[Current task title]"
**Current State:** [What the task currently says]
**Issues:** [What's wrong or unclear]
**Recommended Revision:** [Specific improved task description]
**Rationale:** [Why this improvement is needed]

#### Task X.X: "[Current task title]"
**Current State:** [What the task currently says]
**Issues:** [What's wrong or unclear]
**Recommended Revision:** [Specific improved task description]
**Rationale:** [Why this improvement is needed]

### Tasks Needing Minor Improvements

#### Task X.X: "[Current task title]"
**Suggested Enhancement:** [Minor improvement needed]
**Impact:** [How this improves implementation success]

---

## Risk Assessment Findings

### High-Priority Risks Not Adequately Addressed
1. **[Risk Name]**
   - **Description:** [What could go wrong]
   - **Probability:** [High/Medium/Low]
   - **Impact:** [Potential consequences]
   - **Current Mitigation:** [What's currently planned, if anything]
   - **Recommended Mitigation:** [Additional tasks needed]

### Medium-Priority Risks
1. **[Risk Name]**
   - **Description:** [What could go wrong]
   - **Recommended Tasks:** [Specific mitigation tasks]

---

## Implementation Sequence Analysis

### Critical Path Analysis
**Identified Critical Path:**
[Task X.X] → [Task X.X] → [Task X.X] → [Task X.X]

**Critical Path Issues:**
- [Bottlenecks or problems in the critical path]

**Optimization Opportunities:**
- [How to reduce critical path length or risk]

### Parallel Work Opportunities
**Tasks That Can Be Done Simultaneously:**
- Group 1: [Task X.X, Task X.X, Task X.X]
- Group 2: [Task X.X, Task X.X]

**Currently Sequential Tasks That Could Be Parallel:**
- [Tasks that are unnecessarily sequential]

---

## Quality Gate Recommendations

### Phase 0 Quality Gates (if applicable)
- [ ] [Specific quality check before Phase 1]
- [ ] [Another quality gate]

### Phase 1 Quality Gates
- [ ] [Specific validation before Phase 2]
- [ ] [Another quality gate]

### Phase 2 Quality Gates
- [ ] [Specific validation before Phase 3]
- [ ] [Another quality gate]

### Final Quality Gates
- [ ] [Pre-deployment validation]
- [ ] [Production readiness check]

---

## Improvement Action Plan

### Immediate Actions (Must Complete Before Implementation)
1. **[Action Item]** - [Specific improvement needed]
   - **Owner:** [Who should do this]
   - **Timeline:** [How long it should take]
   - **Success Criteria:** [How to know it's done]

2. **[Action Item]** - [Specific improvement needed]
   - **Owner:** [Who should do this]
   - **Timeline:** [How long it should take]
   - **Success Criteria:** [How to know it's done]

### Important Improvements (Should Complete Before Implementation)
1. **[Action Item]** - [Why it's important]
2. **[Action Item]** - [Why it's important]

### Optional Enhancements (Can Be Done During Implementation)
1. **[Action Item]** - [Why it's beneficial]
2. **[Action Item]** - [Why it's beneficial]

---

## Benchmarking Analysis

### Comparison to Best Practices
**Strengths Relative to Industry Standards:**
- [What this task list does well compared to typical task lists]

**Areas Below Standard:**
- [What this task list lacks compared to best practices]

### Complexity-Appropriate Assessment
**For a [Simple/Medium/Complex] Feature:**
- **Appropriate Depth:** [Aspects that match complexity level]
- **Over-Engineering:** [Areas that might be too detailed]
- **Under-Engineering:** [Areas that need more detail]

---

## Team Readiness Assessment

### Skill Requirements Analysis
**Required Skills Covered:**
- [Skills needed that are likely available]

**Potential Skill Gaps:**
- [Skills that might be missing and need development]

### Workload Distribution
**Well-Balanced Tasks:**
- [Tasks that are appropriately sized]

**Potentially Problematic Tasks:**
- [Tasks that might be too large, too small, or too complex]

---

## Next Steps Recommendation

### If Score ≥ 3.5 (Excellent)
✅ **Ready for immediate implementation**
- Task list provides excellent guidance for successful implementation
- Minor improvements can be made during execution
- Use this task list as a template for similar features

### If Score 3.0-3.4 (Good)
⚠️ **Minor improvements recommended before implementation**
- Address major issues first to reduce implementation risk
- Most tasks are ready for execution
- **Timeline Impact:** 1-2 days for improvements

### If Score 2.5-2.9 (Adequate)
🔄 **Significant improvements needed before implementation**
- Focus on critical and major issues first
- Consider partial implementation of well-defined tasks while improving others
- **Timeline Impact:** 3-5 days for improvements

### If Score < 2.5 (Poor/Unacceptable)
🛑 **Requires substantial rework before implementation**
- Consider regenerating task list with better specifications
- May need to revisit PRD for clarity before creating new tasks
- **Timeline Impact:** 1-2 weeks for complete revision

---

## Quality Improvement Checklist

Before approving task list for implementation:
- [ ] All tasks are actionable with clear verbs and specific targets
- [ ] Dependencies are explicitly identified and logical
- [ ] Each phase has clear deliverables and success criteria
- [ ] Risk mitigation tasks are included for identified risks
- [ ] Testing strategy covers unit, integration, and end-to-end scenarios
- [ ] Validation checkpoints are strategically placed throughout
- [ ] Implementation guidance is sufficient for target team skill level
- [ ] Rollback and recovery procedures are defined for high-risk changes
- [ ] Documentation and knowledge transfer tasks are included
- [ ] Operational readiness tasks prepare for production deployment
```

## Advanced Evaluation Scenarios

### High-Complexity Feature Evaluation

**Additional Assessment Criteria:**
- **Architecture Impact:** Do tasks adequately address system architecture changes?
- **Data Migration:** Are data migration tasks comprehensive and safe?
- **Integration Complexity:** Are external system integrations properly validated?
- **Performance Impact:** Are performance benchmarking and optimization tasks included?
- **Security Review:** Are security checkpoints adequate for the risk level?

**Enhanced Validation Requirements:**
- Comprehensive test coverage including edge cases and failure scenarios
- Performance baseline establishment and regression testing
- Security review and penetration testing
- Stakeholder approval and sign-off processes
- Rollback procedures and disaster recovery planning

### Multi-Team Coordination Evaluation

**Coordination Assessment:**
- **Interface Definitions:** Are team boundaries and responsibilities clear?
- **Communication Protocols:** Are status updates and coordination meetings planned?
- **Dependency Management:** Are cross-team dependencies explicitly managed?
- **Integration Points:** Are team integration and testing activities coordinated?
- **Timeline Synchronization:** Are team deliverables properly sequenced?

### Legacy System Integration Evaluation

**Legacy-Specific Checks:**
- **Compatibility Validation:** Are legacy system constraints properly addressed?
- **Data Format Handling:** Are data transformation and validation tasks included?
- **Gradual Migration:** Are phased migration tasks properly planned?
- **Fallback Procedures:** Are rollback to legacy systems properly planned?
- **Knowledge Transfer:** Are legacy system expertise requirements addressed?

## Integration with Process Workflow

### Recommended Integration Pattern

```
PRD Creation → PRD Quality Evaluation → PRD Iteration (if needed) →
Task Generation → Task Quality Evaluation → Task Iteration (if needed) →
Task Processing with Quality Gates
```

### Quality Gate Integration

**Pre-Implementation Gates:**
- Task list quality score ≥ 3.0 required for implementation start
- Critical issues must be resolved before any coding begins
- Major issues should be resolved before major implementation phases

**During Implementation Gates:**
- Phase completion requires validation task completion
- Quality gate failures trigger task list review and updates
- New tasks discovered during implementation trigger quality re-evaluation

### Continuous Improvement Integration

**Pattern Recognition:**
- Track common task list issues across projects
- Develop project-specific quality checklists
- Build institutional knowledge of effective task patterns

**Template Evolution:**
- Update task generation templates based on evaluation findings
- Create domain-specific task quality standards
- Develop team-specific evaluation criteria

## Usage Guidelines

### When to Use Task Quality Evaluation

**Mandatory Evaluation:**
- Complex features affecting multiple systems
- Features with aggressive timelines or high visibility
- Features being implemented by new or junior team members
- Features with significant technical or business risk
- Features requiring coordination across multiple teams

**Recommended Evaluation:**
- Features in unfamiliar technical domains
- Features with unclear or evolving requirements
- Features with complex integration requirements
- Features with significant performance or security implications

**Optional Evaluation:**
- Simple CRUD operations following established patterns
- Minor bug fixes with small scope
- Features with well-established implementation patterns
- Features being implemented by highly experienced teams

### Evaluation Depth Guidelines

**Light Evaluation (15-30 minutes):**
- Focus on structural organization and critical gaps
- Suitable for simple features with experienced teams
- Emphasize risk identification and validation completeness

**Standard Evaluation (45-90 minutes):**
- Complete assessment across all six dimensions
- Suitable for most features of medium complexity
- Include detailed improvement recommendations

**Deep Evaluation (2-4 hours):**
- Comprehensive analysis with detailed task-by-task review
- Suitable for complex, high-risk, or high-visibility features
- Include benchmarking analysis and team readiness assessment

## Best Practices for Task Quality Evaluation

### Evaluator Excellence Principles

1. **Implementation-First Perspective:** Always evaluate from the viewpoint of someone who will execute these tasks
2. **Risk-Aware Assessment:** Assume Murphy's Law - what can go wrong, will go wrong
3. **Team-Contextual Evaluation:** Consider the actual team that will implement these tasks
4. **Outcome-Focused Analysis:** Judge tasks by their ability to deliver successful outcomes
5. **Continuous Learning Integration:** Use evaluation insights to improve future task generation

### Common Evaluation Pitfalls to Avoid

- **Over-Specification Bias:** Not every simple task needs enterprise-level detail
- **Perfect Planning Fallacy:** Tasks should enable agile adaptation, not prevent it
- **Technical Tunnel Vision:** Consider business context and user impact, not just technical elegance
- **Analysis Paralysis:** Evaluation should enable faster, better implementation, not delay it
- **One-Size-Fits-All Assessment:** Adapt evaluation depth to feature complexity and team context

### Quality vs. Practicality Balance

**Aim for "Good Enough Plus":**
- Tasks should be good enough to ensure success plus a safety margin
- Perfect task lists are less valuable than implementable task lists
- Focus on preventing major failures rather than optimizing minor efficiencies
- Balance thoroughness with development velocity

## Final Integration Notes

This evaluation framework is designed to work seamlessly with your existing workflow:

- **Complements PRD evaluation:** Focuses on implementation readiness rather than requirements quality
- **Enhances task processing:** Provides quality gates that prevent problematic tasks from reaching implementation
- **Supports reflective iteration:** Evaluation findings inform improved task generation for future projects
- **Maintains adaptability:** Evaluation recommendations can be applied during implementation as well as before

The goal is not perfect task lists, but successful feature implementation. A good task list that leads to successful delivery is infinitely more valuable than a perfect task list that never gets implemented.